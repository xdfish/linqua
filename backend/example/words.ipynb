{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Laden des Wortdatensatzes\n",
    "Um mit den Daten von SUBTLEX arbeiten zu können sind einige Schritte der Aufbereitung notwendig.\n",
    "Zu Beginn wird die (Excel-)Datei eingelesen und in einen Pandas Dataframe konvertiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('SUBTLEX-us.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Entfernen von Wörtern mit unbrauchbaren Wortklassen\n",
    "Einige der Wörter wurden Wortklassen zugeordnert welche für unsere Aufgaben uninteressant sind oder nicht sinnvoll in die Aufgabenstellung integriert werden können.\n",
    "So können z.B. einzelne Buchstaben (Letter) genau so wenig für die Aufgabenerstellung verwendet werden wie Wörter denen keine Klasse zugeordnert werden konnte (Unclssified und NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(~df['Dom_PoS_SUBTLEX'].isin(['Letter', 'Unclassified', 'To', 'Not', 'Ex', np.nan]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Entfernen von Wörtern mit nicht eindeutiger Wortklasse\n",
    "Da Wörter mehreren Wortklassen zugehörig sein können (je nach konkreter Verwendung) wird mit dem Wert in Spalte \"Percentage_dom_PoS\" angeben wie häufig das Wort die in der Spalte \"Dom_PoS_SUBTLEX\" angegebene klasse tatsächlich zueghörig ist. Da die Aufgaben später die Vorgabe haben sollen, dass Wörter einer ganz konkreten Wortklasse verwendet sollen, werden in diesem Schritt alle Wörter entfernt die in weniger als 90% der Fälle in der Hauptklasse auftreten. Der Wert von 90% wurde hierbei durch testen ermittelt und hat die besten Ergebnisse erzielt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['Percentage_dom_PoS'] > 0.9)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Normalisieren des SUBTLCD Werts\n",
    "Um im späteren Verlauf besser mit dem Wert der Auftrittswahrscheinlichkeit rechnen zu können wird in diesem Schritt der Wert normalisiert (0 bis 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize SUBTLCD\n",
    "df['SUBTLCD'] = df['SUBTLCD']/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Löschen von ungenutzten Spalten und Umbenennen der relevanten Spalten\n",
    "Für eine bessere Lesbarkeit und Speicherverwaltung werden alle ungenutzten Spalten entfernt, sowie die relevanten Spalten umbenannt.\n",
    "Übrig bleiben so die Spalte \"Word\", welche das konkrete Wort enthält, die Spalte \"Frequency\" (vorher SUBTLCD) welche die Auftrittswahrscheinlichkeit enthält und die Spalte \"Type\" welche die zueghörige Wortklasse repräsentiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[['Word', 'SUBTLCD', 'Dom_PoS_SUBTLEX']].rename(columns={'SUBTLCD':'Frequency', 'Dom_PoS_SUBTLEX':'Type'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Normalisieren der Auftrittswahrscheinlichkeit nach der jeweiligen Wortklasse\n",
    "Da die Auftrittswahrscheinlichkeit wortklassenübergreifend berechnet wurde, gibt es Klassen welche auf Grund ihrer Eigenschaft nur sehr selten in Sätzen verwendet werden. Hierdurch weißen alle zugehörigen Wörter eine niedrige Auftrittswahrscheinlichkeit auf. Um die Auftrittswahrscheinlichkeit einer globalen Schwierigkeit anzunäheren (z.B. Wörter mit einer Frequenz von > 90% sind leicht, Frequnzen von unter 5% hingegen sehr Schwer) werden in diesem Schritt alle Frequenzen so normalisiert, dass jede Wortklasse Frequenzen von 0 bis 1.0 aufweißt.\n",
    "\n",
    "Hierzu wird zuerst die minale und maximale Frequenz der jeweiligen Klasse als Spalte hinzugefügt (calc_min_max).\n",
    "Im zweiten Schritt wird dann in einer neuen Spalte \"FrequencyType\" die Auftrittswahrscheinlichkeit des Worters innerhalb seiner Klasse zwischen 0 (minimal wert) und 1.0 (maximal wert) eingeordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 step\n",
    "def calc_min_max(s):\n",
    "    type_seq = df[(df['Type'] == s['Type'])]['Frequency']\n",
    "    s['Min'] = type_seq.min()\n",
    "    s['Max'] = type_seq.max()\n",
    "    return s\n",
    "\n",
    "word_types_frequency_range = pd.DataFrame({'Type': df['Type'].unique()}).apply(calc_min_max, axis=1)\n",
    "\n",
    "#2 step\n",
    "def calculate_typed_frequency(s):\n",
    "    wtfr = word_types_frequency_range[(word_types_frequency_range['Type'] == s['Type'])]\n",
    "    return ((s['Frequency'] - wtfr['Min'].iloc[0])/(wtfr['Max'].iloc[0] - wtfr['Min'].iloc[0]))\n",
    "\n",
    "df['FrequencyType'] = df.apply(lambda x: calculate_typed_frequency(x), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Index zurücksetzen\n",
    "Als letzten Aufbereitungsschritt wird der Index des DataFrames zurückgesetzt, da durch das löschen einzelner Zeilen entsprechend auch deren Indice entfernt wurde (Pandas spezifisch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINALE FUNKTION ZUM ABRUFEN VON WÖRTER EINER BESTIMMTEN WORTKLASSE\n",
    "\n",
    "### get( )\n",
    "Die Funktion kann nun auf der Basis der zuvor bereitgestellten Wortdaten eine bestimmte Anzahl ('amount') an Wörter einer bestimmten Wortklasse ('word_type') zuückgeben. Mit den Parametern 'min_freq' kann zudem die Frquenz nach unten, mit 'max_freq' entsprechen nach oben beschränkt werden.\n",
    "\n",
    "Will man beispielsweise nur \"leichte\" Wörter kann die minimale Frequenz auf 0.9 bsechränkt werden. Es werden also nur noch Wörter mit einer Auftrittswahrschienlich von 90% inherhalb ihrer eignenen Wortklasse zurückgegeben.\n",
    "\n",
    "Will man \"mittelschwere\" Wörter kann die maximale Frequenz auf 0.9 und die minimale Frequenz auf 0.5 gesetzt werden. Somit werden sehr häufig auftretende Wörter ausgeschlossen (häufiger als 90%), genau wie sehr schwere Wörter (seltener als 50%).\n",
    "\n",
    "### types( )\n",
    "Mit der Funktion werden alle im DataFrame verfügbaren Wortklassen zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(word_type: str, amount: int, min_freq: float = 0.0, max_freq: float = 1.0) -> List[str]:\n",
    "    available_words = df[(df['Type'] == word_type) & (df['FrequencyType'] >= min_freq) & (df['FrequencyType'] <= max_freq)]['Word']\n",
    "    return [word for word in available_words.sample(amount if len(available_words) >= amount else len(available_words))]\n",
    "\n",
    "def types() -> List[str]:\n",
    "    word_types = [t for t in df['Type'].unique()]\n",
    "    word_types.sort()\n",
    "    return word_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 easy adverbs: \tdown, then, maybe, here, please, still, why, never, too, again\n",
      "\n",
      "3 hard nouns: \t\tstiffeners, superimpositions, pattee\n",
      "\n",
      "5 middle verbs: \tcame, excuse, asked, telling, sit\n",
      "\n",
      "available word types: \tAdjective, Adverb, Article, Conjunction, Determiner, Interjection, Name, Noun, Number, Preposition, Pronoun, Verb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Beispiel (Zelle erneut ausführen um neue Worte zu erhalten)\n",
    "\n",
    "# 10 \"einfache\" Adverben\n",
    "ex1 = get(word_type='Adverb', amount=10, min_freq=0.9)\n",
    "print('10 easy adverbs: \\t' + ', '.join(ex1), end='\\n\\n')\n",
    "\n",
    "# 3 \"schwere\" Nomen\n",
    "ex2 = get(word_type='Noun', amount=3, max_freq=0.1)\n",
    "print('3 hard nouns: \\t\\t' + ', '.join(ex2), end='\\n\\n')\n",
    "\n",
    "# 5 \"mittel schwere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Verben\n",
    "ex3 = get(word_type='Verb', amount=5, max_freq=0.9, min_freq=0.5)\n",
    "print('5 middle verbs: \\t' + ', '.join(ex3), end='\\n\\n')\n",
    "\n",
    "# Verfügbare Wortklassen\n",
    "ex4 = types()\n",
    "print('available word types: \\t' + ', '.join(ex4), end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
